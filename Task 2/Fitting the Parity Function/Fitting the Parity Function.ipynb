{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcc5539",
   "metadata": {},
   "source": [
    "## Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b7ae4",
   "metadata": {},
   "source": [
    "The goal of this model is to try and fit the parity function. In other words, we will try to see how accurately can our model act like the Parity Function. The parity function can be defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66986396",
   "metadata": {},
   "source": [
    "$\n",
    "f : x \\in \\{0, 1\\}^{\\otimes n} \\rightarrow y = \n",
    "    \\begin{cases} \n",
    "      \\text{1 if uneven number of 1's in x}\\\\\n",
    "      \\text{0 else} \n",
    "    \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbeffd",
   "metadata": {},
   "source": [
    "As an example, if $x$ is $1100$, the output of the parity function will be $f(1100) = 0$. If $x$ is $1110$, the output of the parity function will be $f(1110) = 1$. We want to train our model in such a way that it acts like the parity function.\n",
    "\n",
    "From the model's perspective, the only thing model has the input and output labels for the bit-strings. It tries to map the input bit-strings to the corresponding output labels by finding the function that suits to the mapping. We do this on a subset of the available dataset. Once the training is done, we evaluate the model based on the other subset to see how accurate the formed mapping function is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165fbe0",
   "metadata": {},
   "source": [
    "## Workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb610d",
   "metadata": {},
   "source": [
    "### 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd0cccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "dev = qml.device('default.qubit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d3a078",
   "metadata": {},
   "source": [
    "Here, we import the requied libraries: $pennylane$ for all the Pennylane functions and $numpy$ for comfortable processing of mathematical data\n",
    "\n",
    "Then, we initialize the device to be a simple-state simulator of qubit-based quantum systems. Other options are $lightning.qubit$, for a more performant system, $default.mixed$ for a mixed-state simulator of qubit-based quantum systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7b986",
   "metadata": {},
   "source": [
    "### 2. Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c80fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preparation(x):\n",
    "    qml.BasisState(x, wires=[i for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555b1b4",
   "metadata": {},
   "source": [
    "The function $state\\_preparation$ takes as input the numpy array which contains the binary representation of the input binary string. For example, for the binary string $010$, the numpy array would be [0 1 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed73d1",
   "metadata": {},
   "source": [
    "The Pennylane function $BasisState$ takes as input, numpy array of the state to be initialized as the first parameter and the wires to which the state is to be assigned as the second parameter. This function can be written in a similar way as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b1abe351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preparation_2(x):\n",
    "    for _, q in enumerate(state):\n",
    "        if q == 1:\n",
    "            qml.PauliX(wires=_)\n",
    "        qml.Identity(wires=_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c3278",
   "metadata": {},
   "source": [
    "From this new function $state\\_preparation\\_2$, we can see that the input numpy array is assigned in the form of MSB of the array to wire_0 and then progressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009561a3",
   "metadata": {},
   "source": [
    "### 3. Variational Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7044e",
   "metadata": {},
   "source": [
    "After we have initialized the input, we apply an operation to the prepared state which is analogous to the operation of assigning weights in classical machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a9c8a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(layer_weights):\n",
    "    for wire in range(len(layer_weights)):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "\n",
    "    for wires in ([[i%(len(layer_weights)+1), (i+1)%(len(layer_weights)+1)] for i in range(len(layer_weights)+1)]):\n",
    "        qml.CNOT(wires)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80c7ea",
   "metadata": {},
   "source": [
    "This operation puts each qubit in an entanglement with its neighboring qubits. This is done by the cyclic application of the $CNOT$ gates. We apply the $Rot$ function to apply a rotation in all the $X, Y, Z$ bases. This is to make sure that the qubit can be generalized and is free to attain any possible state. This make the model very general in its process to find the mapping and is not biased to a particular basis system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb9be6",
   "metadata": {},
   "source": [
    "### 4. Combining the Quantum part of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a873f5f",
   "metadata": {},
   "source": [
    "Having written the functions for state preparation and for the variational block, we can now combine both of them to form a final Quantum block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6497e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "    state_preparation(x)\n",
    "\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153bf99",
   "metadata": {},
   "source": [
    "The function returns the expectation values of each of the possible states for qubit at wire_0 in the $Z$ computational basis. This will prove useful as the $Z$ gate maps $0$ to $1$ and $1$ to $-1$. We can see that the difference is only in the sign of the states and we will use to get the output of the quantum block.\n",
    "\n",
    "We also add a classical parameter $bias$ to the final quantum circuit which also a trainable parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31b02e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831711e3",
   "metadata": {},
   "source": [
    "### 5. Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be856770",
   "metadata": {},
   "source": [
    "Now that the main processing block of the model is done, we look at the training of the model. To do this, we define a classical cost function. This function will tell us how deviated is the result from the correct result. This is also called as the $Loss Function$. There are many types of loss functions. In this example, the standard square loss is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d85f7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b86cc",
   "metadata": {},
   "source": [
    "The cost function is now defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8bc4820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd202ab4",
   "metadata": {},
   "source": [
    "### 6. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb73ee",
   "metadata": {},
   "source": [
    "Once the cost/loss of the predictions is calculated, it is passed through the optimizer to change the $weights$ of the veriational block by certain value. This value is decided by the optimizer. The optimizers try to find the optimum weights by find the local minima of an objective function. In this example, the $NesterovMomentumOptimizer$ is used with a momentum of $0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c027c",
   "metadata": {},
   "source": [
    "We first load the data for the training and evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e77f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"data/parity_train_2.txt\", dtype=int)\n",
    "X = np.array(data[:, :-1])\n",
    "Y = np.array(data[:, -1])\n",
    "Y = Y * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "# for x,y in zip(X, Y):\n",
    "#     print(f\"x = {x}, y = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb526a",
   "metadata": {},
   "source": [
    "We then define the necessary parameters for the weights of the layers, the number of layers, the number of qubits, etc. Initially, we can have the wights as randomly initialized as they will be optimized in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9039a9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[[ 0.01764052  0.00400157  0.00978738]\n",
      "  [ 0.02240893  0.01867558 -0.00977278]\n",
      "  [ 0.00950088 -0.00151357 -0.00103219]\n",
      "  [ 0.00410599  0.00144044  0.01454274]\n",
      "  [ 0.00761038  0.00121675  0.00443863]\n",
      "  [ 0.00333674  0.01494079 -0.00205158]\n",
      "  [ 0.00313068 -0.00854096 -0.0255299 ]\n",
      "  [ 0.00653619  0.00864436 -0.00742165]\n",
      "  [ 0.02269755 -0.01454366  0.00045759]\n",
      "  [-0.00187184  0.01532779  0.01469359]]\n",
      "\n",
      " [[ 0.00154947  0.00378163 -0.00887786]\n",
      "  [-0.01980796 -0.00347912  0.00156349]\n",
      "  [ 0.01230291  0.0120238  -0.00387327]\n",
      "  [-0.00302303 -0.01048553 -0.01420018]\n",
      "  [-0.0170627   0.01950775 -0.00509652]\n",
      "  [-0.00438074 -0.01252795  0.0077749 ]\n",
      "  [-0.01613898 -0.0021274  -0.00895467]\n",
      "  [ 0.00386902 -0.00510805 -0.01180632]\n",
      "  [-0.00028182  0.00428332  0.00066517]\n",
      "  [ 0.00302472 -0.00634322 -0.00362741]]]\n",
      "Bias:  0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = len(X[0])\n",
    "num_layers = 2\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "print(\"Weights:\", weights_init)\n",
    "print(\"Bias: \", bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70d1fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = qml.NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39496b0b",
   "metadata": {},
   "source": [
    "We also set the batch_size parameter which trains the model on a batch of data in an iteration. This makes the training process faster as compared to training one row of data at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89d736",
   "metadata": {},
   "source": [
    "We now define the accuracy of the model as the ratio of the number of inputs for which the correct label was predicted and the total number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43c7d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91cc6885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1 | Cost: 2.0672117 | Accuracy: 0.5226190\n",
      "Iter:    2 | Cost: 1.8848342 | Accuracy: 0.5226190\n",
      "Iter:    3 | Cost: 1.7287990 | Accuracy: 0.5226190\n",
      "Iter:    4 | Cost: 1.1225617 | Accuracy: 0.5226190\n",
      "Iter:    5 | Cost: 1.0243291 | Accuracy: 0.5059524\n",
      "Iter:    6 | Cost: 1.0133340 | Accuracy: 0.5059524\n",
      "Iter:    7 | Cost: 1.0208218 | Accuracy: 0.4940476\n",
      "Iter:    8 | Cost: 1.0001625 | Accuracy: 0.4773810\n",
      "Iter:    9 | Cost: 1.0776565 | Accuracy: 0.4940476\n",
      "Iter:   10 | Cost: 1.0028528 | Accuracy: 0.5059524\n",
      "Iter:   11 | Cost: 1.0029288 | Accuracy: 0.5059524\n",
      "Iter:   12 | Cost: 1.0147975 | Accuracy: 0.5059524\n",
      "Iter:   13 | Cost: 1.0351790 | Accuracy: 0.5059524\n",
      "Iter:   14 | Cost: 1.0061342 | Accuracy: 0.4940476\n",
      "Iter:   15 | Cost: 1.0148540 | Accuracy: 0.5059524\n",
      "Iter:   16 | Cost: 1.0649363 | Accuracy: 0.5059524\n",
      "Iter:   17 | Cost: 1.0029948 | Accuracy: 0.5059524\n",
      "Iter:   18 | Cost: 1.2067436 | Accuracy: 0.5059524\n",
      "Iter:   19 | Cost: 1.0000805 | Accuracy: 0.4940476\n",
      "Iter:   20 | Cost: 1.0000741 | Accuracy: 0.5023810\n",
      "Iter:   21 | Cost: 1.0029362 | Accuracy: 0.5059524\n",
      "Iter:   22 | Cost: 1.0209425 | Accuracy: 0.4940476\n",
      "Iter:   23 | Cost: 1.0000170 | Accuracy: 0.5047619\n",
      "Iter:   24 | Cost: 1.0028895 | Accuracy: 0.5059524\n",
      "Iter:   25 | Cost: 1.0771685 | Accuracy: 0.4940476\n",
      "Iter:   26 | Cost: 1.0447232 | Accuracy: 0.4940476\n",
      "Iter:   27 | Cost: 0.9998807 | Accuracy: 0.5023810\n",
      "Iter:   28 | Cost: 1.0350258 | Accuracy: 0.5059524\n",
      "Iter:   29 | Cost: 1.0027633 | Accuracy: 0.5059524\n",
      "Iter:   30 | Cost: 0.9998199 | Accuracy: 0.5071429\n",
      "Iter:   31 | Cost: 1.0141876 | Accuracy: 0.5059524\n",
      "Iter:   32 | Cost: 1.1177984 | Accuracy: 0.4940476\n",
      "Iter:   33 | Cost: 0.9997604 | Accuracy: 0.5071429\n",
      "Iter:   34 | Cost: 1.0146042 | Accuracy: 0.5059524\n",
      "Iter:   35 | Cost: 0.9997807 | Accuracy: 0.5130952\n",
      "Iter:   36 | Cost: 1.0025111 | Accuracy: 0.5059524\n",
      "Iter:   37 | Cost: 1.0025407 | Accuracy: 0.5059524\n",
      "Iter:   38 | Cost: 1.0055602 | Accuracy: 0.4940476\n",
      "Iter:   39 | Cost: 1.0349217 | Accuracy: 0.5059524\n",
      "Iter:   40 | Cost: 1.0362223 | Accuracy: 0.5059524\n",
      "Iter:   41 | Cost: 1.0631778 | Accuracy: 0.5059524\n",
      "Iter:   42 | Cost: 1.1500158 | Accuracy: 0.5059524\n",
      "Iter:   43 | Cost: 1.0132712 | Accuracy: 0.5059524\n",
      "Iter:   44 | Cost: 1.0202033 | Accuracy: 0.4940476\n",
      "Iter:   45 | Cost: 0.9996777 | Accuracy: 0.5000000\n",
      "Iter:   46 | Cost: 1.0136722 | Accuracy: 0.5059524\n",
      "Iter:   47 | Cost: 1.0056191 | Accuracy: 0.4940476\n",
      "Iter:   48 | Cost: 1.0353808 | Accuracy: 0.5059524\n",
      "Iter:   49 | Cost: 1.0200421 | Accuracy: 0.4940476\n",
      "Iter:   50 | Cost: 1.0146635 | Accuracy: 0.5059524\n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(50):\n",
    "\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "    current_cost = cost(weights, bias, X, Y)\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a14632",
   "metadata": {},
   "source": [
    "Here, we do the training of the model. In each iteration, we divide the input data into batches for training. To make sure that in a given batch, the data is as random as possible, we use numpy's random function to create a batch of 5 rows. These rows are selected by the indices in the list generated by the np.random function.\n",
    "\n",
    "We then optimize the weights and teh bias parameters to find the optimum solution in that iteration for the generated batch of data.\n",
    "\n",
    "Using these parameters, predictions are made and then the cost is calculated. We also measure the accuracy of the model for the inputs and optimized parameters for that iteration.\n",
    "\n",
    "These steps are repeated 100 times. However, this carries the risk of overfitting in the case where the number of iterations is so high to the point where the model has learned the mapping for the input data so well that it can only correctly label the input if the input is one of the training set values. Whereas, if any value which does not belong to the training set is passed, it performs poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade264d2",
   "metadata": {},
   "source": [
    "Therefore, we now have a look at the test set for the evaluation of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "406cc239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 0 0 0 0 1 0 0], y = 1, pred=-1.0\n",
      "x = [0 0 0 0 0 0 0 1 1 1], y = 1, pred=-1.0\n",
      "x = [0 0 0 0 0 1 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [0 0 0 0 1 0 0 0 1 0], y = -1, pred=-1.0\n",
      "x = [0 0 0 0 1 0 0 0 1 1], y = 1, pred=-1.0\n",
      "x = [0 0 0 0 1 0 1 0 1 1], y = -1, pred=-1.0\n",
      "x = [0 0 0 0 1 0 1 1 1 1], y = 1, pred=-1.0\n",
      "x = [0 0 0 0 1 1 0 0 0 1], y = 1, pred=-1.0\n",
      "x = [0 0 0 0 1 1 0 0 1 0], y = 1, pred=-1.0\n",
      "x = [0 0 0 0 1 1 0 0 1 1], y = -1, pred=-1.0\n",
      "x = [0 0 0 0 1 1 0 1 0 0], y = 1, pred=-1.0\n",
      "x = [0 0 0 0 1 1 1 0 0 0], y = 1, pred=-1.0\n",
      "x = [0 0 0 1 0 0 0 1 0 0], y = -1, pred=-1.0\n",
      "x = [0 0 0 1 0 0 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [0 0 0 1 0 0 1 0 1 0], y = 1, pred=-1.0\n",
      "x = [0 0 0 1 0 0 1 0 1 1], y = -1, pred=-1.0\n",
      "x = [0 0 0 1 0 0 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [0 0 0 1 0 1 0 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 0 0 1 0 1 0 1 0 1], y = -1, pred=-1.0\n",
      "x = [0 0 0 1 0 1 1 1 1 1], y = -1, pred=-1.0\n",
      "x = [0 0 0 1 1 0 0 1 1 0], y = -1, pred=-1.0\n",
      "x = [0 0 0 1 1 1 0 0 1 0], y = -1, pred=-1.0\n",
      "x = [0 0 0 1 1 1 0 0 1 1], y = 1, pred=-1.0\n",
      "x = [0 0 0 1 1 1 0 1 1 1], y = -1, pred=-1.0\n",
      "x = [0 0 1 0 0 0 0 0 1 1], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 0 0 0 1 1 0], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 0 0 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 0 0 1 1 1 1], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 0 1 1 0 1 1], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 0 1 1 1 0 1], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 0 1 1 1 1 1], y = -1, pred=-1.0\n",
      "x = [0 0 1 0 1 0 0 1 0 0], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 1 0 1 1 1 1], y = -1, pred=-1.0\n",
      "x = [0 0 1 0 1 1 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 1 1 1 0 1 0], y = 1, pred=-1.0\n",
      "x = [0 0 1 0 1 1 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [0 0 1 1 0 0 1 0 0 0], y = 1, pred=-1.0\n",
      "x = [0 0 1 1 0 0 1 1 0 0], y = -1, pred=-1.0\n",
      "x = [0 0 1 1 0 1 0 1 1 1], y = -1, pred=-1.0\n",
      "x = [0 0 1 1 0 1 1 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 0 1 1 0 1 1 0 1 0], y = 1, pred=-1.0\n",
      "x = [0 0 1 1 0 1 1 1 1 1], y = 1, pred=-1.0\n",
      "x = [0 0 1 1 1 0 0 0 0 1], y = -1, pred=-1.0\n",
      "x = [0 0 1 1 1 1 0 1 0 1], y = -1, pred=-1.0\n",
      "x = [0 0 1 1 1 1 1 0 1 0], y = -1, pred=-1.0\n",
      "x = [0 1 0 0 0 0 0 1 0 1], y = 1, pred=-1.0\n",
      "x = [0 1 0 0 0 1 0 0 1 1], y = -1, pred=-1.0\n",
      "x = [0 1 0 0 0 1 1 0 1 0], y = -1, pred=-1.0\n",
      "x = [0 1 0 0 0 1 1 1 0 0], y = -1, pred=-1.0\n",
      "x = [0 1 0 0 0 1 1 1 0 1], y = 1, pred=-1.0\n",
      "x = [0 1 0 0 1 0 0 1 0 0], y = 1, pred=-1.0\n",
      "x = [0 1 0 0 1 0 0 1 1 1], y = 1, pred=-1.0\n",
      "x = [0 1 0 0 1 0 1 0 0 0], y = 1, pred=-1.0\n",
      "x = [0 1 0 0 1 0 1 0 1 1], y = 1, pred=-1.0\n",
      "x = [0 1 0 0 1 1 0 1 1 0], y = 1, pred=-1.0\n",
      "x = [0 1 0 0 1 1 1 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 1 0 0 1 1 1 1 1 1], y = 1, pred=-1.0\n",
      "x = [0 1 0 1 0 0 1 1 1 0], y = 1, pred=-1.0\n",
      "x = [0 1 0 1 0 1 0 0 0 1], y = -1, pred=-1.0\n",
      "x = [0 1 0 1 0 1 0 1 0 0], y = -1, pred=-1.0\n",
      "x = [0 1 0 1 0 1 1 1 1 1], y = 1, pred=-1.0\n",
      "x = [0 1 0 1 1 0 0 0 0 0], y = 1, pred=-1.0\n",
      "x = [0 1 0 1 1 0 1 0 1 0], y = 1, pred=-1.0\n",
      "x = [0 1 0 1 1 1 0 0 1 1], y = -1, pred=-1.0\n",
      "x = [0 1 0 1 1 1 0 1 0 0], y = 1, pred=-1.0\n",
      "x = [0 1 0 1 1 1 0 1 0 1], y = -1, pred=-1.0\n",
      "x = [0 1 0 1 1 1 0 1 1 1], y = 1, pred=-1.0\n",
      "x = [0 1 0 1 1 1 1 0 0 0], y = 1, pred=-1.0\n",
      "x = [0 1 0 1 1 1 1 1 0 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 0 0 0 1 0 1 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 0 1 0 0 0 1 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 0 1 0 1 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 0 1 0 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [0 1 1 0 1 0 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 0 1 1 0 0 1 0], y = 1, pred=-1.0\n",
      "x = [0 1 1 0 1 1 0 1 0 1], y = -1, pred=-1.0\n",
      "x = [0 1 1 1 0 0 1 1 0 0], y = 1, pred=-1.0\n",
      "x = [0 1 1 1 0 0 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 1 0 1 0 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 1 0 1 0 1 1 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 1 0 1 1 1 0 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 1 1 0 0 0 1 1], y = -1, pred=-1.0\n",
      "x = [0 1 1 1 1 0 1 0 1 1], y = 1, pred=-1.0\n",
      "x = [0 1 1 1 1 1 0 1 1 0], y = 1, pred=-1.0\n",
      "x = [0 1 1 1 1 1 1 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 1 1 1 1 1 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 0 0 0 0 0 0 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 0 0 0 0 0 0 1 0 0], y = -1, pred=-1.0\n",
      "x = [1 0 0 0 0 0 0 1 1 0], y = 1, pred=-1.0\n",
      "x = [1 0 0 0 0 0 1 0 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 0 0 1 0 0 0 0], y = -1, pred=-1.0\n",
      "x = [1 0 0 0 0 1 0 0 1 0], y = 1, pred=-1.0\n",
      "x = [1 0 0 0 0 1 0 0 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 0 1 0 0 1 1 1], y = 1, pred=-1.0\n",
      "x = [1 0 0 0 1 0 1 0 0 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 0 1 0 1 1 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 0 1 1 1 0 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 0 1 1 1 1 0 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 1 0 0 0 0 1 0], y = 1, pred=-1.0\n",
      "x = [1 0 0 1 0 0 1 1 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 1 0 1 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 0 0 1 0 1 1 1 0 0], y = 1, pred=-1.0\n",
      "x = [1 0 0 1 0 1 1 1 1 1], y = 1, pred=-1.0\n",
      "x = [1 0 0 1 1 0 0 0 0 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 1 1 0 0 1 0 1], y = 1, pred=-1.0\n",
      "x = [1 0 0 1 1 0 1 0 0 0], y = -1, pred=-1.0\n",
      "x = [1 0 0 1 1 0 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 0 0 1 1 0 1 1 0 1], y = -1, pred=-1.0\n",
      "x = [1 0 0 1 1 0 1 1 1 1], y = 1, pred=-1.0\n",
      "x = [1 0 0 1 1 1 1 0 0 0], y = 1, pred=-1.0\n",
      "x = [1 0 0 1 1 1 1 1 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 1 0 0 0 0 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 0 1 0 0 0 0 1 1 0], y = -1, pred=-1.0\n",
      "x = [1 0 1 0 0 0 1 0 1 1], y = 1, pred=-1.0\n",
      "x = [1 0 1 0 0 1 0 1 0 1], y = 1, pred=-1.0\n",
      "x = [1 0 1 0 0 1 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [1 0 1 0 1 0 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 0 1 0 1 1 0 0 1 0], y = 1, pred=-1.0\n",
      "x = [1 0 1 0 1 1 0 0 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 1 0 1 1 1 0 0 1], y = -1, pred=-1.0\n",
      "x = [1 0 1 0 1 1 1 1 0 1], y = 1, pred=-1.0\n",
      "x = [1 0 1 0 1 1 1 1 1 0], y = 1, pred=-1.0\n",
      "x = [1 0 1 1 0 0 0 0 0 0], y = 1, pred=-1.0\n",
      "x = [1 0 1 1 0 0 0 1 1 0], y = 1, pred=-1.0\n",
      "x = [1 0 1 1 0 0 1 0 0 0], y = -1, pred=-1.0\n",
      "x = [1 0 1 1 0 1 0 1 1 0], y = -1, pred=-1.0\n",
      "x = [1 0 1 1 0 1 0 1 1 1], y = 1, pred=-1.0\n",
      "x = [1 0 1 1 0 1 1 0 1 1], y = 1, pred=-1.0\n",
      "x = [1 0 1 1 1 0 0 1 0 1], y = -1, pred=-1.0\n",
      "x = [1 0 1 1 1 0 1 0 0 0], y = 1, pred=-1.0\n",
      "x = [1 0 1 1 1 0 1 0 0 1], y = -1, pred=-1.0\n",
      "x = [1 0 1 1 1 0 1 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 0 1 1 1 1 0 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 0 1 1 1 1 1 0 1 1], y = -1, pred=-1.0\n",
      "x = [1 1 0 0 0 0 0 0 0 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 0 0 0 1 1 1 0], y = 1, pred=-1.0\n",
      "x = [1 1 0 0 0 1 0 1 1 0], y = 1, pred=-1.0\n",
      "x = [1 1 0 0 0 1 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 0 1 0 0 0 0 0], y = 1, pred=-1.0\n",
      "x = [1 1 0 0 1 0 1 0 0 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 0 1 0 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 0 1 1 0 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 1 0 0 1 1 1 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 0 1 1 1 0 1 1], y = 1, pred=-1.0\n",
      "x = [1 1 0 0 1 1 1 1 0 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 1 0 0 1 0 0 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 1 0 1 0 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 0 1 0 0 1 0], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 0 1 1 0 0 1], y = -1, pred=-1.0\n",
      "x = [1 1 0 1 0 1 1 0 1 1], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 0 1 1 1 0 1], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 0 1 1 1 1 0], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 0 1 1 1 1 1], y = -1, pred=-1.0\n",
      "x = [1 1 0 1 1 0 0 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 1 0 0 1 0 0], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 1 0 0 1 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 1 1 0 1 1 0 1], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 1 1 0 1 1 1], y = -1, pred=-1.0\n",
      "x = [1 1 0 1 1 1 1 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 1 0 1 1 1 1 0 1 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 0 0 0 0 0 0 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 0 0 0 0 1 1 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 0 0 0 1 1 0 1], y = -1, pred=-1.0\n",
      "x = [1 1 1 0 0 1 1 1 1 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 0 1 0 0 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 1 1 0 1 0 0 1 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 1 0 1 0 0 1 1 1], y = 1, pred=-1.0\n",
      "x = [1 1 1 0 1 0 1 0 0 1], y = -1, pred=-1.0\n",
      "x = [1 1 1 0 1 0 1 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 1 0 1 1 0 0 1 1], y = 1, pred=-1.0\n",
      "x = [1 1 1 0 1 1 1 1 0 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 0 1 1 1 1 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 1 1 0 0 0 1 0 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 1 0 0 1 0 1 1], y = 1, pred=-1.0\n",
      "x = [1 1 1 1 0 0 1 1 1 1], y = -1, pred=-1.0\n",
      "x = [1 1 1 1 0 1 0 0 0 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 1 1 0 0 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 1 1 1 0 1 1 1 1], y = 1, pred=-1.0\n",
      "x = [1 1 1 1 1 1 0 0 0 1], y = 1, pred=-1.0\n",
      "x = [1 1 1 1 1 1 0 0 1 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 1 1 1 0 1 0 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 1 1 1 0 1 0 1], y = -1, pred=-1.0\n",
      "x = [1 1 1 1 1 1 1 0 0 0], y = 1, pred=-1.0\n",
      "x = [1 1 1 1 1 1 1 0 0 1], y = -1, pred=-1.0\n",
      "Accuracy on unseen data: 0.47282608695652173\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/parity_test_2.txt\", dtype=int)\n",
    "X_test = np.array(data[:, :-1])\n",
    "Y_test = np.array(data[:, -1])\n",
    "Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "for x,y,p in zip(X_test, Y_test, predictions_test):\n",
    "    print(f\"x = {x}, y = {y}, pred={p}\")\n",
    "\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"Accuracy on unseen data:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee89fe",
   "metadata": {},
   "source": [
    "In this case, the accuracy of prediction on unseen data is $0.47282608695652173$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bf099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
